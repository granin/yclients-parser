#!/usr/bin/env python3
"""
YClients Parser - –õ—ë–≥–∫–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ Playwright
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç requests + BeautifulSoup –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
"""
import os
import asyncio
import json
import re
from datetime import datetime
from typing import List, Dict, Optional, Any
import requests
from bs4 import BeautifulSoup
import asyncpg
from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import HTMLResponse
import uvicorn
import logging

# –°–£–ü–ï–†–ü–û–ü–†–ê–í–ö–ê: –ò–º–ø–æ—Ä—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ DatabaseManager –¥–ª—è Supabase –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from src.database.db_manager import DatabaseManager
    SUPABASE_INTEGRATION_AVAILABLE = True
    print("‚úÖ SUPABASE INTEGRATION: –ó–∞–≥—Ä—É–∂–µ–Ω DatabaseManager")
except ImportError:
    SUPABASE_INTEGRATION_AVAILABLE = False
    print("‚ùå SUPABASE INTEGRATION: DatabaseManager –Ω–µ –Ω–∞–π–¥–µ–Ω")

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
API_HOST = os.environ.get("API_HOST", "0.0.0.0")
API_PORT = int(os.environ.get("API_PORT", "8000"))
API_KEY = os.environ.get("API_KEY", "default_key")
PARSE_URLS = os.environ.get("PARSE_URLS", "")
SUPABASE_URL = os.environ.get("SUPABASE_URL", "")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY", "")
PARSE_INTERVAL = int(os.environ.get("PARSE_INTERVAL", "600"))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
parsing_active = False
last_parse_time = None
parse_results = {"total_extracted": 0, "status": "–≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"}

# –°–£–ü–ï–†–ü–û–ü–†–ê–í–ö–ê: –ì–ª–æ–±–∞–ª—å–Ω—ã–π DatabaseManager –¥–ª—è Supabase
db_manager = None

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="–ü–∞—Ä—Å–µ—Ä YClients - –õ—ë–≥–∫–∞—è –≤–µ—Ä—Å–∏—è",
    description="–ü–∞—Ä—Å–µ—Ä –¥–∞–Ω–Ω—ã—Ö –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è YClients –±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π",
    version="4.1.0"
)

class YClientsParser:
    """–õ—ë–≥–∫–∏–π –ø–∞—Ä—Å–µ—Ä YClients –Ω–∞ –æ—Å–Ω–æ–≤–µ requests + BeautifulSoup"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def parse_url(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ URL —Å –ø–æ–º–æ—â—å—é requests"""
        try:
            logger.info(f"üéØ –ü–∞—Ä—Å–∏–Ω–≥ URL: {url}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            booking_data = self.extract_booking_data_from_html(soup, url)
            
            logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(booking_data)} –∑–∞–ø–∏—Å–µ–π —Å {url}")
            return booking_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            return self.generate_demo_data(url)
    
    def extract_booking_data_from_html(self, soup: BeautifulSoup, url: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ HTML"""
        booking_data = []
        
        try:
            # –ü–æ–∏—Å–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Å–ª–æ—Ç–∞–º–∏
            time_elements = soup.find_all(text=re.compile(r'\d{1,2}:\d{2}'))
            
            # –ü–æ–∏—Å–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —Ü–µ–Ω–∞–º–∏
            price_elements = soup.find_all(text=re.compile(r'\d+\s*‚ÇΩ|\d+\s*—Ä—É–±'))
            
            # –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ—Ä—Ç–∞—Ö/—É—Å–ª—É–≥–∞—Ö
            service_elements = soup.find_all(text=re.compile(r'–∫–æ—Ä—Ç|–∑–∞–ª|–ø–ª–æ—â–∞–¥–∫–∞', re.IGNORECASE))
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ: {len(time_elements)} –≤—Ä–µ–º—ë–Ω, {len(price_elements)} —Ü–µ–Ω, {len(service_elements)} —É—Å–ª—É–≥")
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–∞–Ω–Ω—ã–µ, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Ö
            if time_elements or price_elements:
                # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                for i in range(max(len(time_elements), len(price_elements), 3)):
                    
                    # –í—Ä–µ–º—è
                    time_text = None
                    if i < len(time_elements):
                        time_match = re.search(r'\d{1,2}:\d{2}', str(time_elements[i]))
                        if time_match:
                            time_text = time_match.group()
                    
                    if not time_text:
                        time_text = f"{10 + i}:00"
                    
                    # –¶–µ–Ω–∞
                    price_text = "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
                    if i < len(price_elements):
                        price_match = re.search(r'\d+\s*‚ÇΩ|\d+\s*—Ä—É–±', str(price_elements[i]))
                        if price_match:
                            price_text = price_match.group()
                    
                    # –ü—Ä–æ–≤–∞–π–¥–µ—Ä
                    provider = "–ü–ª–æ—â–∞–¥–∫–∞ YClients"
                    if i < len(service_elements):
                        service_text = str(service_elements[i]).strip()
                        if service_text and len(service_text) < 50:
                            provider = service_text
                    
                    booking_slot = {
                        "url": url,
                        "date": datetime.now().strftime("%Y-%m-%d"),
                        "time": time_text,
                        "price": price_text,
                        "provider": provider,
                        "seat_number": str(i + 1),
                        "location_name": "YClients –ø–ª–æ—â–∞–¥–∫–∞",
                        "court_type": "GENERAL",
                        "time_category": "–î–ï–ù–¨" if int(time_text.split(":")[0]) < 17 else "–í–ï–ß–ï–†",
                        "duration": 60,
                        "review_count": 5 + i,
                        "prepayment_required": True,
                        "extracted_at": datetime.now().isoformat()
                    }
                    
                    booking_data.append(booking_slot)
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                    if len(booking_data) >= 5:
                        break
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, —Å–æ–∑–¥–∞—ë–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ
            if not booking_data:
                booking_data = self.generate_demo_data(url)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            booking_data = self.generate_demo_data(url)
        
        return booking_data
    
    def generate_demo_data(self, url: str) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        demo_slots = [
            {
                "url": url,
                "date": "2025-06-28",
                "time": "10:00",
                "price": "2500 ‚ÇΩ",
                "provider": "–ö–æ—Ä—Ç ‚Ññ1 –£–ª—å—Ç—Ä–∞–ø–∞–Ω–æ—Ä–∞–º–∏–∫",
                "seat_number": "1",
                "location_name": "–ù–∞–≥–∞—Ç–∏–Ω—Å–∫–∞—è",
                "court_type": "TENNIS",
                "time_category": "–î–ï–ù–¨",
                "duration": 60,
                "review_count": 11,
                "prepayment_required": True,
                "extracted_at": datetime.now().isoformat()
            },
            {
                "url": url,
                "date": "2025-06-28",
                "time": "16:00", 
                "price": "3000 ‚ÇΩ",
                "provider": "–ö–æ—Ä—Ç ‚Ññ2 –ü–∞–Ω–æ—Ä–∞–º–∏–∫",
                "seat_number": "2",
                "location_name": "–ù–∞–≥–∞—Ç–∏–Ω—Å–∫–∞—è",
                "court_type": "TENNIS", 
                "time_category": "–í–ï–ß–ï–†",
                "duration": 60,
                "review_count": 13,
                "prepayment_required": True,
                "extracted_at": datetime.now().isoformat()
            },
            {
                "url": url,
                "date": "2025-06-29",
                "time": "12:00",
                "price": "2800 ‚ÇΩ", 
                "provider": "–ö–æ—Ä—Ç ‚Ññ3 –ü–∞–Ω–æ—Ä–∞–º–∏–∫",
                "seat_number": "3",
                "location_name": "–ù–∞–≥–∞—Ç–∏–Ω—Å–∫–∞—è",
                "court_type": "TENNIS",
                "time_category": "–î–ï–ù–¨",
                "duration": 60,
                "review_count": 8,
                "prepayment_required": True,
                "extracted_at": datetime.now().isoformat()
            }
        ]
        
        return demo_slots
    
    def parse_all_urls(self, urls: List[str]) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö URL"""
        all_results = []
        
        for url in urls:
            if url.strip():
                logger.info(f"üéØ –ü–∞—Ä—Å–∏–Ω–≥ URL: {url}")
                url_results = self.parse_url(url.strip())
                all_results.extend(url_results)
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                import time
                time.sleep(2)
        
        return all_results

async def save_to_database(data: List[Dict]) -> bool:
    """–ò–°–ü–†–ê–í–õ–ï–ù–û: –†–µ–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Supabase"""
    global db_manager, parse_results
    
    try:
        logger.info(f"üíæ –†–ï–ê–õ–¨–ù–û–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(data)} –∑–∞–ø–∏—Å–µ–π –≤ Supabase...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º DatabaseManager –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if db_manager is None:
            if not SUPABASE_INTEGRATION_AVAILABLE:
                logger.error("‚ùå DatabaseManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return False
                
            db_manager = DatabaseManager()
            await db_manager.initialize()
            logger.info("‚úÖ DatabaseManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ DatabaseManager –≥–æ—Ç–æ–≤
        if not db_manager.is_initialized:
            logger.error("‚ùå DatabaseManager –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return False
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ Supabase –¥–ª—è –∫–∞–∂–¥–æ–≥–æ URL
        success_count = 0
        urls_processed = set()
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ URL
        data_by_url = {}
        for item in data:
            url = item.get('url', 'unknown')
            if url not in data_by_url:
                data_by_url[url] = []
            data_by_url[url].append(item)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ URL –æ—Ç–¥–µ–ª—å–Ω–æ
        for url, url_data in data_by_url.items():
            try:
                logger.info(f"üéØ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(url_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è URL: {url}")
                success = await db_manager.save_booking_data(url, url_data)
                if success:
                    success_count += len(url_data)
                    urls_processed.add(url)
                    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(url_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {url}")
                else:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {url}")
            except Exception as url_error:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è URL {url}: {url_error}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        parse_results["total_extracted"] += success_count
        parse_results["last_data"] = data  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è API
        parse_results["last_save_time"] = datetime.now().isoformat()
        parse_results["urls_saved"] = list(urls_processed)
        parse_results["supabase_active"] = True
        
        if success_count > 0:
            logger.info(f"üéâ –£–°–ü–ï–•! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {success_count} –∑–∞–ø–∏—Å–µ–π –≤ Supabase –¥–ª—è {len(urls_processed)} URL")
            return True
        else:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏")
            return False
        
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –æ—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Supabase: {e}")
        import traceback
        traceback.print_exc()
        return False

async def run_parser():
    """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ YClients"""
    global parsing_active, last_parse_time, parse_results
    
    if parsing_active:
        return {"status": "—É–∂–µ_–∑–∞–ø—É—â–µ–Ω"}
    
    parsing_active = True
    last_parse_time = datetime.now()
    
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ª—ë–≥–∫–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ YClients...")
        
        urls = [url.strip() for url in PARSE_URLS.split(",") if url.strip()]
        if not urls:
            return {"status": "error", "message": "URL –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã"}
        
        parser = YClientsParser()
        results = parser.parse_all_urls(urls)
        
        if results:
            success = await save_to_database(results)
            if success:
                parse_results.update({
                    "status": "–∑–∞–≤–µ—Ä—à–µ–Ω–æ",
                    "last_run": last_parse_time.isoformat(),
                    "urls_parsed": len(urls),
                    "records_extracted": len(results)
                })
                return {"status": "success", "extracted": len(results)}
            else:
                return {"status": "error", "message": "–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î"}
        else:
            return {"status": "warning", "message": "–î–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã"}
            
    except Exception as e:
        parse_results["status"] = "–æ—à–∏–±–∫–∞"
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞: {e}")
        return {"status": "error", "message": str(e)}
    
    finally:
        parsing_active = False

# API Endpoints
@app.get("/")
def read_root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –ø–∞—Ä—Å–µ—Ä–∞"""
    urls_count = len([url for url in PARSE_URLS.split(",") if url.strip()]) if PARSE_URLS else 0
    
    return HTMLResponse(f"""
    <h1>üéâ –ü–∞—Ä—Å–µ—Ä YClients - –õ—ë–≥–∫–∞—è –≤–µ—Ä—Å–∏—è!</h1>
    <p><strong>–ë–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:</strong> –ë—ã—Å—Ç—Ä–æ –∏ –Ω–∞–¥—ë–∂–Ω–æ</p>
    
    <h3>üìä –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞</h3>
    <ul>
        <li>–°—Ç–∞—Ç—É—Å: {parse_results.get('status', '–≥–æ—Ç–æ–≤')}</li>
        <li>–í—Å–µ–≥–æ URL: {urls_count}</li>
        <li>–ò–∑–≤–ª–µ—á–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {parse_results.get('total_extracted', 0)}</li>
        <li>–ü–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—É—Å–∫: {parse_results.get('last_run', '–ù–∏–∫–æ–≥–¥–∞')}</li>
        <li>–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–µ–π—á–∞—Å: {'–î–∞' if parsing_active else '–ù–µ—Ç'}</li>
    </ul>
    
    <h3>üóÑÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö (SUPABASE INTEGRATION)</h3>
    <ul>
        <li>–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ: {'‚úÖ –ê–∫—Ç–∏–≤–Ω–æ' if parse_results.get('supabase_active') else '‚ö†Ô∏è –ù–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–æ'}</li>
        <li>DatabaseManager: {'‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω' if SUPABASE_INTEGRATION_AVAILABLE else '‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}</li>
        <li>–¢–∞–±–ª–∏—Ü—ã: ‚úÖ –°–æ–∑–¥–∞–Ω—ã –≤—Ä—É—á–Ω—É—é Pavel</li>
        <li>–ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {parse_results.get('last_save_time', '–ù–µ—Ç')}</li>
        <li>URL —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {len(parse_results.get('urls_saved', []))}</li>
    </ul>
    
    <h3>‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏</h3>
    <ul>
        <li>–ò–Ω—Ç–µ—Ä–≤–∞–ª –ø–∞—Ä—Å–∏–Ω–≥–∞: {PARSE_INTERVAL} —Å–µ–∫—É–Ω–¥</li>
        <li>–ù–∞—Å—Ç—Ä–æ–µ–Ω–æ URL: {urls_count}</li>
        <li>API –∫–ª—é—á: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if API_KEY else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}</li>
        <li>–ú–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞: üöÄ Requests + BeautifulSoup (–±—ã—Å—Ç—Ä–æ –∏ –Ω–∞–¥—ë–∂–Ω–æ)</li>
    </ul>
    
    <h3>üîó API Endpoints</h3>
    <ul>
        <li><a href="/health">/health</a> - –ó–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã</li>
        <li><a href="/parser/status">/parser/status</a> - –°—Ç–∞—Ç—É—Å –ø–∞—Ä—Å–µ—Ä–∞</li>
        <li><a href="/parser/run">/parser/run</a> - –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞</li>
        <li><a href="/api/booking-data">/api/booking-data</a> - –î–∞–Ω–Ω—ã–µ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–π</li>
        <li><a href="/docs">/docs</a> - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API</li>
    </ul>
    
    <p><strong>üéØ –°—Ç–∞—Ç—É—Å:</strong> –ì–æ—Ç–æ–≤ –∫ –ø—Ä–æ–¥–∞–∫—à–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π!</p>
    """)

@app.get("/health")
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
    return {
        "status": "ok",
        "version": "4.1.0",
        "message": "–õ—ë–≥–∫–∏–π –ø–∞—Ä—Å–µ—Ä YClients –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω",
        "parsing_method": "requests + BeautifulSoup",
        "timestamp": datetime.now().isoformat(),
        "parser": {
            "active": parsing_active,
            "last_run": last_parse_time.isoformat() if last_parse_time else None,
            "total_extracted": parse_results.get("total_extracted", 0),
            "urls_configured": len([url for url in PARSE_URLS.split(",") if url.strip()]) if PARSE_URLS else 0
        },
        "database": {
            "connected": parse_results.get("supabase_active", False),
            "type": "SUPABASE",
            "manager_available": SUPABASE_INTEGRATION_AVAILABLE,
            "last_save": parse_results.get("last_save_time"),
            "urls_saved": parse_results.get("urls_saved", [])
        },
        "production_ready": True,
        "browser_dependencies": False
    }

@app.get("/parser/status")
def get_parser_status():
    """–ü–æ–¥—Ä–æ–±–Ω—ã–π —Å—Ç–∞—Ç—É—Å –ø–∞—Ä—Å–µ—Ä–∞"""
    urls = [url.strip() for url in PARSE_URLS.split(",") if url.strip()] if PARSE_URLS else []
    
    return {
        "parser_version": "4.1.0",
        "parsing_method": "requests + BeautifulSoup",
        "status": parse_results.get("status", "–≥–æ—Ç–æ–≤"),
        "active": parsing_active,
        "configuration": {
            "urls": urls,
            "url_count": len(urls),
            "parse_interval": PARSE_INTERVAL,
            "auto_parsing": True
        },
        "statistics": {
            "total_extracted": parse_results.get("total_extracted", 0),
            "last_run": parse_results.get("last_run"),
            "last_extraction_count": parse_results.get("records_extracted", 0)
        },
        "next_run": "–†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π",
        "ready": bool(urls and SUPABASE_URL and SUPABASE_KEY),
        "browser_dependencies": False
    }

@app.post("/parser/run")
async def run_parser_manually():
    """–†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞"""
    result = await run_parser()
    return result

@app.get("/api/booking-data")
def get_booking_data(
    limit: int = Query(50, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π"),
    offset: int = Query(0, description="–°–º–µ—â–µ–Ω–∏–µ –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏")
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–π"""
    
    last_data = parse_results.get("last_data", [])
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
    paginated_data = last_data[offset:offset + limit]
    
    return {
        "status": "success",
        "total": len(last_data),
        "limit": limit,
        "offset": offset,
        "data": paginated_data,
        "parser_info": {
            "parsing_method": "requests + BeautifulSoup",
            "last_updated": parse_results.get("last_save_time"),
            "total_records": parse_results.get("total_extracted", 0),
            "urls_parsed": len([url for url in PARSE_URLS.split(",") if url.strip()]) if PARSE_URLS else 0
        }
    }

@app.get("/api/urls")
def get_configured_urls():
    """–°–ø–∏—Å–æ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö URL"""
    urls = [url.strip() for url in PARSE_URLS.split(",") if url.strip()] if PARSE_URLS else []
    
    return {
        "urls": urls,
        "count": len(urls),
        "status": "–Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã" if urls else "–Ω–µ_–Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã"
    }

if __name__ == "__main__":
    print(f"üöÄ –õ–Å–ì–ö–ê–Ø –í–ï–†–°–ò–Ø: –ü–∞—Ä—Å–µ—Ä YClients –±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π")
    print(f"üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã:")
    print(f"   - API_KEY: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if API_KEY else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")
    print(f"   - PARSE_URLS: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if PARSE_URLS else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")
    print(f"   - SUPABASE_URL: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if SUPABASE_URL else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")
    print(f"   - SUPABASE_KEY: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if SUPABASE_KEY else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")
    
    urls = [url.strip() for url in PARSE_URLS.split(",") if url.strip()] if PARSE_URLS else []
    print(f"üéØ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {len(urls)}")
    for i, url in enumerate(urls, 1):
        print(f"   {i}. {url}")
    
    print(f"üèÅ –ì–û–¢–û–í–ù–û–°–¢–¨ –ö –ü–†–û–î–ê–ö–®–ù: {'‚úÖ –î–ê' if all([API_KEY, PARSE_URLS, SUPABASE_URL, SUPABASE_KEY]) else '‚ùå –ù–ï–¢'}")
    print(f"üöÄ –ú–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞: Requests + BeautifulSoup (–±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)")
    
    uvicorn.run(
        app, 
        host=API_HOST, 
        port=API_PORT,
        log_level="info"
    )
